{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 6 : Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load data for binary classification\n",
    "binary_classification_Y_pred = np.load('binary_classification_Y_pred.npy')\n",
    "binary_classification_Y_true = np.load('binary_classification_Y_true.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Our classes are as follows:\n",
    "- Positive Class is 0\n",
    "- Negative class is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    \"\"\"Compute the confusion matrix for binary classification\n",
    "    The target class is assumed to be the positive class. (i.e. 0 is positive, 1 is negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        np.array: confusion matrix\n",
    "    \"\"\"\n",
    "    #extract the classed from the data\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    #loop through the classes to get the tp,tn,fp,fn\n",
    "    for i in range(len(Y_pred)):\n",
    "        if Y_true[i]==1 and Y_pred[i]==1:\n",
    "            TN+=1\n",
    "        if Y_pred[i]==1 and Y_true[i]!=Y_pred[i]:\n",
    "            FN+=1    \n",
    "        if Y_true[i]==Y_pred[i]==0:\n",
    "            TP +=1\n",
    "        if Y_pred[i]==0 and Y_true[i]!=Y_pred[i]:\n",
    "            FP+=1\n",
    "    \n",
    "    \n",
    "    # return the confusion matrix\n",
    "    \n",
    "    \n",
    "\n",
    "    # do not change this line\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 18]\n",
      " [20 48]]\n",
      "True Positives: 93\n",
      "False Positives: 18\n",
      "False Negatives: 20\n",
      "True Negatives: 48\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred))\n",
    "\n",
    "TP, FP, FN, TN = confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred).ravel()\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n",
    "print(f\"True Negatives: {TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your True Positives: 93 - Sklearn True Positives: 93\n",
      "Your False Negatives: 20 - Sklearn False Negatives: 20\n",
      "Your False Positives: 18 - Sklearn False Positives: 18\n",
      "Your True Negatives: 48 - Sklearn True Negatives: 48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "\n",
    "SK_TN, SK_FP, SK_FN, SK_TP = sk_confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred, labels=[1,0]).ravel()\n",
    "print(f\"Your True Positives: {TP} - Sklearn True Positives: {SK_TP}\")\n",
    "print(f\"Your False Negatives: {FN} - Sklearn False Negatives: {SK_FN}\")\n",
    "print(f\"Your False Positives: {FP} - Sklearn False Positives: {SK_FP}\")\n",
    "print(f\"Your True Negatives: {TN} - Sklearn True Negatives: {SK_TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(Y_true, Y_pred):\n",
    "    \"\"\"calculate precision\n",
    "    Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: precision score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def recall(Y_true, Y_pred):\n",
    "    \"\"\"calculate recall\n",
    "    Recall = True Positive / (True Positive + False Negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: recall score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "def f1_score(Y_true, Y_pred):\n",
    "    \"\"\"calculate f1 score\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return f1_score\n",
    "\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    \"\"\"calculate accuracy\n",
    "    Accuracy = (True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: accuracy score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure that the output of your functions match that of sklearn.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m recall_score \u001b[39mas\u001b[39;00m sk_recall_score\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnsure that the output of your functions match that of sklearn.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYour precision score: \u001b[39m\u001b[39m{\u001b[39;00mprecision(binary_classification_Y_true, binary_classification_Y_pred)\u001b[39m}\u001b[39;00m\u001b[39m - sklearn\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms precision score: \u001b[39m\u001b[39m{\u001b[39;00msk_precision_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYour recall score: \u001b[39m\u001b[39m{\u001b[39;00mrecall(binary_classification_Y_true, binary_classification_Y_pred)\u001b[39m}\u001b[39;00m\u001b[39m - sklearn\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms recall score: \u001b[39m\u001b[39m{\u001b[39;00msk_recall_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYour f1 score: \u001b[39m\u001b[39m{\u001b[39;00mf1_score(binary_classification_Y_true, binary_classification_Y_pred)\u001b[39m}\u001b[39;00m\u001b[39m - sklearn\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms f1 score: \u001b[39m\u001b[39m{\u001b[39;00msk_f1_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision' is not defined"
     ]
    }
   ],
   "source": [
    "# Checking for correct results\n",
    "# Checking for correct results\n",
    "\n",
    "from sklearn.metrics import f1_score as sk_f1_score\n",
    "from sklearn.metrics import accuracy_score as sk_accuracy_score\n",
    "from sklearn.metrics import precision_score as sk_precision_score\n",
    "from sklearn.metrics import recall_score as sk_recall_score\n",
    "print(\"Ensure that the output of your functions match that of sklearn.\")\n",
    "print(f\"Your precision score: {precision(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's precision score: {sk_precision_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your recall score: {recall(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's recall score: {sk_recall_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your f1 score: {f1_score(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's f1 score: {sk_f1_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your accuracy score: {accuracy(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's accuracy score: {sk_accuracy_score(binary_classification_Y_true, binary_classification_Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_Y_pred = np.load('regression_Y_pred.npy')\n",
    "regression_Y_true = np.load('regression_Y_true.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean absolute error\n",
    "    MAE = 1/n * sum(|Y_true - Y_pred|)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean absolute error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return mae_score\n",
    "\n",
    "def mean_squared_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean squared error\n",
    "    MSE = 1/n * sum((Y_true - Y_pred)^2)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean squared error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return mse_score\n",
    "\n",
    "def root_mean_squared_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate root mean squared error\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: root mean squared error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return rmse_score\n",
    "\n",
    "def mean_absolute_percentage_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean absolute percentage error\n",
    "    MAPE = 1/n * sum(|(Y_true - Y_pred)/Y_true|)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean absolute percentage error\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return mape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correct results\n",
    "from sklearn.metrics import mean_absolute_error as sk_mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as sk_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error as sk_mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "print(\"Ensure that the output of your functions match that of sklearn.\")\n",
    "print(f\"Your mean absolute error: {mean_absolute_error(regression_Y_true, regression_Y_pred)} - sklearn's mean absolute error: {sk_mean_absolute_error(regression_Y_true, regression_Y_pred)}\")\n",
    "print(f\"Your mean squared error: {mean_squared_error(regression_Y_true, regression_Y_pred)} - sklearn's mean squared error: {sk_mean_squared_error(regression_Y_true, regression_Y_pred)}\")\n",
    "print(f\"Your root mean squared error: {root_mean_squared_error(regression_Y_true, regression_Y_pred)} - sklearn's root mean squared error: {np.sqrt(sk_mean_squared_error(regression_Y_true, regression_Y_pred))}\")\n",
    "print(f\"Your mean absolute percentage error: {mean_absolute_percentage_error(regression_Y_true, regression_Y_pred)} - sklearn's mean absolute percentage error: {sk_mean_absolute_percentage_error(regression_Y_true, regression_Y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95606bc53929b54f756dff225d66a5ebf14d3425b29085c482108fea047c442b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
