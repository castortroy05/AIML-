{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Week 6 : Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load data for binary classification\n",
    "binary_classification_Y_pred = np.load('binary_classification_Y_pred.npy')\n",
    "binary_classification_Y_true = np.load('binary_classification_Y_true.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Our classes are as follows:\n",
    "- Positive Class is 0\n",
    "- Negative class is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    \"\"\"Compute the confusion matrix for binary classification\n",
    "    The target class is assumed to be the positive class. (i.e. 0 is positive, 1 is negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        np.array: confusion matrix\n",
    "    \"\"\"\n",
    "    #extract the classed from the data\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    #loop through the classes to get the tp,tn,fp,fn\n",
    "    for i in range(len(Y_pred)):\n",
    "        if Y_true[i]==1 and Y_pred[i]==1:\n",
    "            TN+=1\n",
    "        if Y_pred[i]==1 and Y_true[i]!=Y_pred[i]:\n",
    "            FN+=1    \n",
    "        if Y_true[i]==Y_pred[i]==0:\n",
    "            TP +=1\n",
    "        if Y_pred[i]==0 and Y_true[i]!=Y_pred[i]:\n",
    "            FP+=1\n",
    "    \n",
    "    \n",
    "    # return the confusion matrix\n",
    "    \n",
    "    \n",
    "\n",
    "    # do not change this line\n",
    "    return np.array([[TP, FP], [FN, TN]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 18]\n",
      " [20 48]]\n",
      "True Positives: 93\n",
      "False Positives: 18\n",
      "False Negatives: 20\n",
      "True Negatives: 48\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred))\n",
    "\n",
    "TP, FP, FN, TN = confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred).ravel()\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n",
    "print(f\"True Negatives: {TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your True Positives: 93 - Sklearn True Positives: 93\n",
      "Your False Negatives: 20 - Sklearn False Negatives: 20\n",
      "Your False Positives: 18 - Sklearn False Positives: 18\n",
      "Your True Negatives: 48 - Sklearn True Negatives: 48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "\n",
    "SK_TN, SK_FP, SK_FN, SK_TP = sk_confusion_matrix(binary_classification_Y_true, binary_classification_Y_pred, labels=[1,0]).ravel()\n",
    "print(f\"Your True Positives: {TP} - Sklearn True Positives: {SK_TP}\")\n",
    "print(f\"Your False Negatives: {FN} - Sklearn False Negatives: {SK_FN}\")\n",
    "print(f\"Your False Positives: {FP} - Sklearn False Positives: {SK_FP}\")\n",
    "print(f\"Your True Negatives: {TN} - Sklearn True Negatives: {SK_TN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "def precision(Y_true, Y_pred):\n",
    "    \"\"\"calculate precision\n",
    "    Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: precision score\n",
    "    \"\"\"\n",
    "    #get the tp,fp,fn,tn using numpy\n",
    "    TP, FP, FN, TN = confusion_matrix(Y_true, Y_pred).ravel()\n",
    "    \n",
    "    \n",
    "    #calculate the precision score using numpy\n",
    "    precision = TP/(TP+FP)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def recall(Y_true, Y_pred):\n",
    "    \"\"\"calculate recall\n",
    "    Recall = True Positive / (True Positive + False Negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: recall score\n",
    "    \"\"\"\n",
    "    #calculate the recall\n",
    "    recall_score = TP/(TP+FN)\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "def f1_score(Y_true, Y_pred):\n",
    "    \"\"\"calculate f1 score\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score\n",
    "    \"\"\"\n",
    "    #assign the precision and recall to variables\n",
    "    \n",
    "    precision = precision_score(Y_true, Y_pred)\n",
    "    recall = recall_score(Y_true, Y_pred)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    return f1_score\n",
    "\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    \"\"\"calculate accuracy\n",
    "    Accuracy = (True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: accuracy score\n",
    "    \"\"\"\n",
    "    accuracy_score = (TP+TN)/(TP+TN+FP+FN)\n",
    "   \n",
    "    return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure that the output of your functions match that of sklearn.\n",
      "Your precision score: <function precision_score at 0x7fd9e76da820> - sklearn's precision score: 0.8378378378378378\n",
      "Your recall score: 0.8230088495575221 - sklearn's recall score: 0.8230088495575221\n",
      "Your f1 score: 0.7164179104477613 - sklearn's f1 score: 0.8303571428571428\n",
      "Your accuracy score: 0.7877094972067039 - sklearn's accuracy score: 0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "# Checking for correct results\n",
    "# Checking for correct results\n",
    "\n",
    "from sklearn.metrics import f1_score as sk_f1_score\n",
    "from sklearn.metrics import accuracy_score as sk_accuracy_score\n",
    "from sklearn.metrics import precision_score as sk_precision_score\n",
    "from sklearn.metrics import recall_score as sk_recall_score\n",
    "print(\"Ensure that the output of your functions match that of sklearn.\")\n",
    "print(f\"Your precision score: {precision(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's precision score: {sk_precision_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your recall score: {recall(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's recall score: {sk_recall_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your f1 score: {f1_score(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's f1 score: {sk_f1_score(binary_classification_Y_true, binary_classification_Y_pred, pos_label = 0)}\")\n",
    "print(f\"Your accuracy score: {accuracy(binary_classification_Y_true, binary_classification_Y_pred)} - sklearn's accuracy score: {sk_accuracy_score(binary_classification_Y_true, binary_classification_Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_Y_pred = np.load('regression_Y_pred.npy')\n",
    "regression_Y_true = np.load('regression_Y_true.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean absolute error\n",
    "    MAE = 1/n * sum(|Y_true - Y_pred|)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean absolute error\n",
    "    \"\"\"\n",
    "    #calculate the mean absolute error\n",
    "    mae_score = np.sum(np.abs(Y_true-Y_pred))/len(Y_true)\n",
    "    return mae_score\n",
    "\n",
    "def mean_squared_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean squared error\n",
    "    MSE = 1/n * sum((Y_true - Y_pred)^2)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean squared error\n",
    "    \"\"\"\n",
    "    #calculate the mean squared error\n",
    "    mse_score = np.sum((Y_true-Y_pred)**2)/len(Y_true)\n",
    "    return mse_score\n",
    "\n",
    "def root_mean_squared_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate root mean squared error\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: root mean squared error\n",
    "    \"\"\"\n",
    "    #calculate the root mean squared error\n",
    "    rmse_score = np.sqrt(np.sum((Y_true-Y_pred)**2)/len(Y_true))\n",
    "    return rmse_score\n",
    "\n",
    "def mean_absolute_percentage_error(Y_true, Y_pred):\n",
    "    \"\"\"calculate mean absolute percentage error\n",
    "    MAPE = 1/n * sum(|(Y_true - Y_pred)/Y_true|)\n",
    "\n",
    "    Args:\n",
    "        Y_true (np.array): true labels\n",
    "        Y_pred (np.array): predicted labels\n",
    "\n",
    "    Returns:\n",
    "        float: mean absolute percentage error\n",
    "    \"\"\"\n",
    "    #calculate the mean absolute percentage error\n",
    "    mape_score = np.sum(np.abs((Y_true-Y_pred)/Y_true))/len(Y_true)\n",
    "    return mape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure that the output of your functions match that of sklearn.\n",
      "Your mean absolute error: 1.7286112369635167 - sklearn's mean absolute error: 1.7286112369635167\n",
      "Your mean squared error: 5.523588632282228 - sklearn's mean squared error: 5.523588632282228\n",
      "Your root mean squared error: 2.3502316124761466 - sklearn's root mean squared error: 2.3502316124761466\n",
      "Your mean absolute percentage error: 0.07561512979625772 - sklearn's mean absolute percentage error: 0.07561512979625772\n"
     ]
    }
   ],
   "source": [
    "# Checking for correct results\n",
    "from sklearn.metrics import mean_absolute_error as sk_mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error as sk_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error as sk_mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "print(\"Ensure that the output of your functions match that of sklearn.\")\n",
    "print(f\"Your mean absolute error: {mean_absolute_error(regression_Y_true, regression_Y_pred)} - sklearn's mean absolute error: {sk_mean_absolute_error(regression_Y_true, regression_Y_pred)}\")\n",
    "print(f\"Your mean squared error: {mean_squared_error(regression_Y_true, regression_Y_pred)} - sklearn's mean squared error: {sk_mean_squared_error(regression_Y_true, regression_Y_pred)}\")\n",
    "print(f\"Your root mean squared error: {root_mean_squared_error(regression_Y_true, regression_Y_pred)} - sklearn's root mean squared error: {np.sqrt(sk_mean_squared_error(regression_Y_true, regression_Y_pred))}\")\n",
    "print(f\"Your mean absolute percentage error: {mean_absolute_percentage_error(regression_Y_true, regression_Y_pred)} - sklearn's mean absolute percentage error: {sk_mean_absolute_percentage_error(regression_Y_true, regression_Y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95606bc53929b54f756dff225d66a5ebf14d3425b29085c482108fea047c442b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
